{"cells":[{"cell_type":"markdown","id":"51d3e6d1-4ce0-45fe-9296-60b9ec8c4841","metadata":{},"source":["<p style=\"text-align:center\">\n","    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01\" target=\"_blank\">\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n","    </a>\n","</p>\n"]},{"cell_type":"markdown","id":"899990e4-6229-4ae5-aec2-9ab97d21a5a1","metadata":{},"source":["# **Course Rating Prediction using Neural Networks**\n"]},{"cell_type":"markdown","id":"3a590f18-88b6-4678-a3c6-c1073879795a","metadata":{},"source":["Estimated time needed: **60** minutes\n"]},{"cell_type":"markdown","id":"bc07888c-a937-4390-891c-8f159a5945d1","metadata":{},"source":["In the previous labs, we have crafted several types of user and item feature vectors.  For example, given a user `i`, we may build its profile feature vector and course rating feature vector, and given an item `j`, we may create its genre vector and user enrollment vectors.\n"]},{"cell_type":"markdown","id":"30b75483-34e1-460e-ae57-b563b5d8f5f2","metadata":{},"source":["\n","With these explicit features vectors, we can perform machine learning tasks such as calculating the similarities among users or items, finding nearest neighbors, and using dot-product to estimate a rating value. \n","\n","The main advantage of using these explicit features is they are highly interpretable and yield very good performance as well. The main disadvantage is we need to spend quite some effort to build and store them.\n"]},{"cell_type":"markdown","id":"241cdfc2-4ede-4e4f-a763-d7a226e12b47","metadata":{},"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_4/images/explicit_user_item_features.png)\n"]},{"cell_type":"markdown","id":"634fb4c3-cd1a-4c80-afc5-b01ab7c88d5d","metadata":{},"source":["Is it possible to predict a rating without building explicit feature vectors beforehand?  \n","\n","Yes, as you may recall, the Non-negative Matrix Factorization decomposes the user-item interaction matrix into user matrix and item matrix, which contain the latent features of users and items and you can simply dot-product them to get an estimated rating.\n"]},{"cell_type":"markdown","id":"91a938f1-3720-4b5e-abcf-1e7bb7f5f113","metadata":{},"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_4/images/nmf.png)\n"]},{"cell_type":"markdown","id":"623f49f0-2cf7-4094-8884-783f0980d31e","metadata":{},"source":["In addition to NMF, neural networks can also be used to extract the latent user and item features?  In fact,  neural networks are very good at learning patterns from data and are widely used to extract latent features.  When training neural networks, it gradually captures and stores the features within its hidden layers as weight matrices and can be extracted to represent the original data.\n"]},{"cell_type":"markdown","id":"259d6fe8-09fe-4311-9fd4-55606155a44f","metadata":{},"source":["In this lab, you will be training neural networks to predict course ratings while simultaneously extracting users' and items' latent features. \n"]},{"cell_type":"markdown","id":"35bb7589-d223-4f0c-8aaa-9340c2a0d61a","metadata":{},"source":["## Objectives\n"]},{"cell_type":"markdown","id":"f4f1e1d9-0569-43f2-a8db-c7daae9da4cf","metadata":{},"source":["After completing this lab you will be able to:\n"]},{"cell_type":"markdown","id":"b309b802-74f3-40e8-a7aa-dafcdc323d05","metadata":{},"source":["* Use `tensorflow` to train neural networks to extract the user and item latent features from the hidden's layers  \n","* Predict course ratings with trained neural networks\n"]},{"cell_type":"markdown","id":"7a474d4d-efbd-45f8-b165-b47e93887504","metadata":{},"source":["----\n"]},{"cell_type":"markdown","id":"e5132dfd-3f42-4867-ac16-771736dd9d6b","metadata":{},"source":["## Prepare and setup lab environment\n"]},{"cell_type":"markdown","id":"d7f4714c-c0ad-4816-a309-6f0fd42d08c4","metadata":{},"source":["Install tensorflow 2.7 if not installed before in your Python environment\n"]},{"cell_type":"markdown","id":"bffd4abc-08e0-4845-bb45-d601408c3c14","metadata":{},"source":["and import required libraries:\n"]},{"cell_type":"code","execution_count":26,"id":"d2cca8ba-dab5-4617-95c9-4ff14184b4bd","metadata":{},"outputs":[],"source":["import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import torch\n","import numpy as np\n","from math import sqrt\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":27,"id":"46b1eca5-4dcc-4f69-b865-f3aa2dde0b52","metadata":{},"outputs":[],"source":["# also set a random state\n","rs = 123"]},{"cell_type":"markdown","id":"3df30857-26fb-4dd3-8edd-1372c205455e","metadata":{},"source":["### Load and processing rating dataset\n"]},{"cell_type":"code","execution_count":28,"id":"f0cbf852-4a5e-4205-aa6d-b3bae89a0c2e","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user</th>\n","      <th>item</th>\n","      <th>rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1889878</td>\n","      <td>CC0101EN</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1342067</td>\n","      <td>CL0101EN</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1990814</td>\n","      <td>ML0120ENv3</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>380098</td>\n","      <td>BD0211EN</td>\n","      <td>3.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>779563</td>\n","      <td>DS0101EN</td>\n","      <td>3.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      user        item  rating\n","0  1889878    CC0101EN     3.0\n","1  1342067    CL0101EN     3.0\n","2  1990814  ML0120ENv3     3.0\n","3   380098    BD0211EN     3.0\n","4   779563    DS0101EN     3.0"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["rating_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/datasets/ratings.csv\"\n","rating_df = pd.read_csv(rating_url)\n","rating_df.head()"]},{"cell_type":"markdown","id":"a14e1a19-3b7d-41dd-9b68-3b9041c33b2d","metadata":{},"source":["This is the same rating dataset we have been using in previous lab, which contains the three main columns: `user`, `item`, and `rating`. \n"]},{"cell_type":"markdown","id":"924fbce1-cb1e-4fb4-a991-f77348d88577","metadata":{},"source":["Next, let's figure out how many unique users and items, their total numbers will determine the sizes of one-hot encoding vectors.\n"]},{"cell_type":"code","execution_count":29,"id":"01912ceb-ab1d-4600-a79f-60c21a44f1e5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["There are total `33901` of users and `126` items\n"]}],"source":["num_users = len(rating_df['user'].unique())\n","num_items = len(rating_df['item'].unique())\n","print(f\"There are total `{num_users}` of users and `{num_items}` items\")"]},{"cell_type":"markdown","id":"6bcd9652-e882-401f-9e23-56c6301b3588","metadata":{},"source":["It means our each user can be represented as a `33901 x 1` one-hot vector and each item can be represented as `126 x 1` one-hot vector.\n"]},{"cell_type":"markdown","id":"f3db37e9-14f7-4426-a72a-e8091ad0f11c","metadata":{},"source":["The goal is to create a neural network structure that can take the user and item one-hot vectors as inputs and outputs a rating estimation or the probability of interaction (such as the probability of completing a course). \n","\n","While training and updating the weights in the neural network, its hidden layers should be able to capture the pattern or features for each user and item. Based on this idea, we can design a simple neural network architecture like the following:\n"]},{"cell_type":"markdown","id":"06d380d7-9dc1-476e-91d9-c37a2056c854","metadata":{},"source":["![](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML321EN-SkillsNetwork/labs/module_4/images/embedding_feature_vector.png)\n"]},{"cell_type":"markdown","id":"cb9f4159-a687-4672-b532-91a771752fb7","metadata":{},"source":["The network inputs are two one-hot encoding vectors, the blue one is for the user and the green one is for the item. Then on top of them, we added two embedding layers. Here embedding means embedding the one-hot encoding vector into a latent feature space. The embedding layer is a fully-connected layer that outputs the embedding feature vectors. For example, the user embedding layer takes `33901 x 1` one-hot vector as input and outputs a `16 x 1` embedding vector.\n"]},{"cell_type":"markdown","id":"54d4b720-935b-4950-8c59-49143744cadf","metadata":{},"source":["The embedding layer outputs two embedding vectors, which are similar to Non-negative matrix factorization. Then we could simply dot the product the user and item embedding vector to output a rating estimation.\n"]},{"cell_type":"markdown","id":"483a3a53-3269-4e7f-8b77-de8f63ed1265","metadata":{},"source":["#### Implementing the recommender neural network using tensorflow \n"]},{"cell_type":"markdown","id":"302aa955-b032-4c3f-9248-d90776ba48a5","metadata":{},"source":["This network architecture could be defined and implemented as a sub-class inheriting the `tensorflow.keras.Model` super class, let's call it `RecommenderNet()`.\n"]},{"cell_type":"code","execution_count":30,"id":"faf22412-aa0b-4903-baf0-656e9d6cd4a9","metadata":{},"outputs":[],"source":["class RecommenderNet(nn.Module):\n","    \"\"\"\n","    PyTorch version of the Keras RecommenderNet.\n","    \"\"\"\n","    def __init__(self, num_users, num_items, embedding_size=16):\n","        super(RecommenderNet, self).__init__()\n","        self.user_embedding = nn.Embedding(num_users, embedding_size)\n","        self.item_embedding = nn.Embedding(num_items, embedding_size)\n","        self.user_bias      = nn.Embedding(num_users, 1)\n","        self.item_bias      = nn.Embedding(num_items, 1)\n","\n","        # He‐normal initialization for embeddings\n","        nn.init.kaiming_normal_(self.user_embedding.weight, nonlinearity='relu')\n","        nn.init.kaiming_normal_(self.item_embedding.weight, nonlinearity='relu')\n","\n","    def forward(self, user_idx, item_idx):\n","        u_b   = self.user_bias(user_idx).squeeze() # (B,)\n","        i_b   = self.item_bias(item_idx).squeeze() # (B,)\n","        u_vec = self.user_embedding(user_idx)      # (B, E)\n","        i_vec = self.item_embedding(item_idx)      # (B, E)\n","        dot   = (u_vec * i_vec).sum(dim=1)         # (B,)\n","        x     = dot + u_b + i_b\n","        return torch.relu(x)\n"]},{"cell_type":"markdown","id":"96e243d9-323f-486f-a0b1-ed3c365d50ab","metadata":{},"source":["### TASK: Train and evaluate the RecommenderNet()\n"]},{"cell_type":"markdown","id":"52a8447a-3ee0-4b9a-bb00-c8bc26590c22","metadata":{},"source":["Now it's time to train and evaluate the defined `RecommenderNet()`. First, we need to process the original rating dataset a little bit by converting the actual user ids and item ids into integer indices for `tensorflow` to creating the one-hot encoding vectors.\n"]},{"cell_type":"code","execution_count":31,"id":"e367b70c-7373-4fc3-88c0-dee452c0f6bf","metadata":{},"outputs":[],"source":["def process_dataset(raw_data):\n","    \n","    encoded_data = raw_data.copy()\n","    \n","    # Mapping user ids to indices\n","    user_list = encoded_data[\"user\"].unique().tolist()\n","    user_id2idx_dict = {x: i for i, x in enumerate(user_list)}\n","    user_idx2id_dict = {i: x for i, x in enumerate(user_list)}\n","    \n","    # Mapping course ids to indices\n","    course_list = encoded_data[\"item\"].unique().tolist()\n","    course_id2idx_dict = {x: i for i, x in enumerate(course_list)}\n","    course_idx2id_dict = {i: x for i, x in enumerate(course_list)}\n","\n","    # Convert original user ids to idx\n","    encoded_data[\"user\"] = encoded_data[\"user\"].map(user_id2idx_dict)\n","    # Convert original course ids to idx\n","    encoded_data[\"item\"] = encoded_data[\"item\"].map(course_id2idx_dict)\n","    # Convert rating to int\n","    encoded_data[\"rating\"] = encoded_data[\"rating\"].values.astype(\"int\")\n","\n","    return encoded_data, user_idx2id_dict, course_idx2id_dict"]},{"cell_type":"code","execution_count":32,"id":"fd94664f-010a-4af4-86b5-68aaa35351b7","metadata":{},"outputs":[],"source":["encoded_data, user_idx2id_dict, course_idx2id_dict = process_dataset(rating_df)"]},{"cell_type":"code","execution_count":33,"id":"30aa1b91-bf81-4b84-a684-1f9dc29ac8d4","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>user</th>\n","      <th>item</th>\n","      <th>rating</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>2</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   user  item  rating\n","0     0     0       3\n","1     1     1       3\n","2     2     2       3\n","3     3     3       3\n","4     4     4       3"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["encoded_data.head()"]},{"cell_type":"markdown","id":"04702641-1bf0-46fb-a6d8-61846dc88137","metadata":{},"source":["Then we can split the encoded dataset into training and testing datasets.\n"]},{"cell_type":"code","execution_count":34,"id":"a920aacf-ea34-4809-ba7d-133ffbb0d802","metadata":{},"outputs":[],"source":["def generate_train_test_datasets(dataset, scale=True, random_state=42):\n","    \"\"\"\n","    Splits into train (70%), val (10%), test (20%) and (optionally) scales\n","    the train/val ratings. Returns 6 arrays: X_train, X_val, X_test,\n","    y_train, y_val, y_test.\n","    \"\"\"\n","    # 1) Features & target\n","    X = dataset[['user', 'item']].values\n","    y = dataset['rating'].values\n","\n","    # 2) Hold out 20% for test\n","    X_train_val, X_test, y_train_val, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=random_state, shuffle=True\n","    )\n","\n","    # 3) Split the remaining 80% into 70% train / 10% val\n","    #    10% of original is 0.125 of the 80%\n","    X_train, X_val, y_train, y_val = train_test_split(\n","        X_train_val, y_train_val, test_size=0.125,\n","        random_state=random_state, shuffle=True\n","    )\n","\n","    # 4) Scale train & val only\n","    if scale:\n","        scaler = MinMaxScaler()\n","        # reshape to (-1,1) for scaler, then flatten back\n","        y_train = scaler.fit_transform(y_train.reshape(-1, 1)).ravel()\n","        y_val   = scaler.transform(   y_val.reshape(-1,   1)).ravel()\n","        # leave y_test unscaled\n","\n","    # 5) Return exactly six arrays\n","    return X_train, X_val, X_test, y_train, y_val, y_test\n","\n","# Now this will work without the 2D-array error:\n","x_train, x_val, x_test, y_train, y_val, y_test = generate_train_test_datasets(encoded_data)\n"]},{"cell_type":"code","execution_count":35,"id":"2ad47858-1b60-40e9-a028-b242765cffa0","metadata":{},"outputs":[],"source":["x_train, x_val, x_test, y_train, y_val, y_test = generate_train_test_datasets(encoded_data)"]},{"cell_type":"markdown","id":"defd2055-d0ae-4472-9e26-1d503907d202","metadata":{},"source":["If we take a look at the training input data, it is simply just a list of user indices and item indices, which is a dense format of one-hot encoding vectors.\n"]},{"cell_type":"code","execution_count":36,"id":"3e3c1292-d606-42f8-bd68-0a48d55b1369","metadata":{},"outputs":[{"data":{"text/plain":["array([ 6483, 10636,  1219, ...,  7055,  1072, 20911])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["user_indices = x_train[:, 0]\n","user_indices"]},{"cell_type":"code","execution_count":37,"id":"e2684228-e188-48c6-a0a9-c65f4e0fc8c0","metadata":{},"outputs":[{"data":{"text/plain":["array([ 8, 17, 22, ..., 15, 22,  0])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["item_indices = x_train[:, 1]\n","item_indices"]},{"cell_type":"markdown","id":"b2db4f39-1bdf-4a00-85ce-b9151076397f","metadata":{},"source":["and training output labels are a list of 0s and 1s indicating if the user has completed a course or not.\n"]},{"cell_type":"code","execution_count":38,"id":"565ed3ff-f47e-47a6-8557-89a96a4350c8","metadata":{},"outputs":[{"data":{"text/plain":["array([1., 1., 1., ..., 1., 1., 1.])"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["y_train"]},{"cell_type":"markdown","id":"a987a6f4-a738-4bd1-a787-03486866398d","metadata":{},"source":["Then we can choose a small embedding vector size to be 16 and create a `RecommenderNet()` model to be trained\n"]},{"cell_type":"code","execution_count":39,"id":"3a01a64a-0631-44e2-b7a2-33b87ca81835","metadata":{},"outputs":[],"source":["embedding_size = 16\n","model = RecommenderNet(num_users, num_items, embedding_size)"]},{"cell_type":"markdown","id":"38ddd9f5-80a2-4461-a52f-96fbce46e834","metadata":{},"source":["_TODO: Train the RecommenderNet() model_\n"]},{"cell_type":"code","execution_count":44,"id":"7bbe604f-ddba-49f3-a1aa-5cbd354be597","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1 → Train RMSE: 0.7961\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[44], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, r)\n\u001b[1;32m     31\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     35\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["## WRITE YOUR CODE HERE:\n","\n","## - call model.compile() method to set up the loss and optimizer and metrics for the model training, you may use\n","##  - - tf.keras.losses.MeanSquaredError() as training loss\n","##  - - keras.optimizers.Adam() as optimizer\n","##  - - tf.keras.metrics.RootMeanSquaredError() as metric\n","\n","## - call model.fit() to train the model\n","\n","## - optionally call model.save() to save the model\n","\n","## - plot the train and validation loss\n","import math\n","model     = RecommenderNet(len(user_idx2id_dict), len(course_idx2id_dict), embedding_size=16)\n","criterion = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","\n","# 2. Training & validation loop\n","n_epochs = 20\n","for epoch in range(1, n_epochs+1):\n","    # — Training (no explicit shuffle)\n","    total_loss = 0.0\n","    model.train()\n","    for (u_i, i_i), rating in zip(x_train, y_train):\n","        u = torch.tensor([u_i], dtype=torch.long)\n","        i = torch.tensor([i_i], dtype=torch.long)\n","        r = torch.tensor([rating], dtype=torch.float32)\n","\n","        pred = model(u, i)\n","        loss = criterion(pred, r)\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","    train_rmse = math.sqrt(total_loss / len(x_train))\n","    # …same for validation…\n","    print(f\"Epoch {epoch} → Train RMSE: {train_rmse:.4f}\")\n","\n","\n"]},{"cell_type":"markdown","id":"460e7fd3-e9c6-414d-a589-544c67588f09","metadata":{},"source":["<details>\n","    <summary>Click here for Hints</summary>\n","    \n","When you are fitting a model, dont forget to specify the parameters: `x=x_train, y=y_train`, as well as `batch_size=64`, number of `epochs=10` and of course `validation_data=(x_val, y_val)` you can also define `verbose = 1` which will show you an animated progress for the training progress for each epoch.\n","    \n","* You can set  `history = model.fit()` which will give you a \"loss\" dataframe which will be very useful for ploting the train and validation loss. To plot it, use plt.plot() with `history.history[\"loss\"]` as its parameter for train loss and `history.history[\"val_loss\"]` for validation loss.\n"]},{"cell_type":"markdown","id":"46692d80-a3ff-4783-aa20-292e5274cb22","metadata":{},"source":["_TODO:_ Evaluate the trained model\n"]},{"cell_type":"code","execution_count":48,"id":"c0fd2b01-feb6-4ae8-916d-1e2874c4f32f","metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[48], line 32\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, n_epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# … your training code …\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     train_rmse \u001b[38;5;241m=\u001b[39m evaluate(x_train, y_train, model, criterion)\n\u001b[0;32m---> 32\u001b[0m     val_rmse   \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m — Train RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_rmse\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[48], line 20\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(X, y, model, criterion)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;66;03m# forward + loss\u001b[39;00m\n\u001b[1;32m     19\u001b[0m         pred \u001b[38;5;241m=\u001b[39m model(u_tensor, i_tensor)\n\u001b[0;32m---> 20\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m mse  \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m n_samples\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/nn/modules/loss.py:610\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/nn/functional.py:3905\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction, weight)\u001b[0m\n\u001b[1;32m   3901\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3902\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid reduction mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreduction\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3903\u001b[0m         )\n\u001b[1;32m   3904\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3905\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpanded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpanded_target\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["def evaluate(X, y, model, criterion):\n","    \"\"\"\n","    Compute RMSE of model on the (X,y) dataset.\n","    X: array of shape (N,2) with [user_idx, item_idx]\n","    y: array of shape (N,) with true (possibly scaled) ratings\n","    \"\"\"\n","    model.eval()\n","    total_loss = 0.0\n","    n_samples  = len(X)\n","    \n","    with torch.no_grad():\n","        for (u_i, i_i), rating in zip(X, y):\n","            # build single‐sample tensors\n","            u_tensor = torch.tensor([u_i], dtype=torch.long)\n","            i_tensor = torch.tensor([i_i], dtype=torch.long)\n","            r_tensor = torch.tensor([rating], dtype=torch.float32)\n","            \n","            # forward + loss\n","            pred = model(u_tensor, i_tensor)\n","            loss = criterion(pred, r_tensor)\n","            total_loss += loss.item()\n","    \n","    mse  = total_loss / n_samples\n","    rmse = math.sqrt(mse)\n","    return rmse\n","\n","# Example usage inside your training loop:\n","for epoch in range(1, n_epochs+1):\n","    # … your training code …\n","\n","    train_rmse = evaluate(x_train, y_train, model, criterion)\n","    val_rmse   = evaluate(x_val,   y_val,   model, criterion)\n","    print(f\"Epoch {epoch}/{n_epochs} — Train RMSE: {train_rmse:.4f}, Val RMSE: {val_rmse:.4f}\")\n"]},{"cell_type":"markdown","id":"7bf34bc7-1352-4ad0-894b-15c8e63fa236","metadata":{},"source":["<details>\n","    <summary>Click here for Hints</summary>\n","    \n","Use `x_test, y_test` as parameters for `model.evaluate()`\n"]},{"cell_type":"markdown","id":"6b2dcae8-dcdf-48cf-9a1e-b2837685f135","metadata":{},"source":["### Extract the user and item embedding vectors as latent feature vectors\n"]},{"cell_type":"markdown","id":"a59b09e9-4ad5-41c7-be0f-881bfc001854","metadata":{},"source":["Now, we have trained the `RecommenderNet()` model and it can predict the ratings with relatively small RMSE. \n","\n","If we print the trained model then we can see its layers and their parameters/weights.\n"]},{"cell_type":"code","execution_count":null,"id":"8b3f3726-55e8-4fda-b9d2-6e69b9ac9bda","metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'RecommenderNet' object has no attribute 'summary'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m()\n","File \u001b[0;32m~/Desktop/NLP/.conda/lib/python3.12/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n","\u001b[0;31mAttributeError\u001b[0m: 'RecommenderNet' object has no attribute 'summary'"]}],"source":["model.summary()"]},{"cell_type":"markdown","id":"db95bc15-44e4-471f-a06a-d621e766022b","metadata":{},"source":["In the `RecommenderNet`, the `user_embedding_layer` and `item_embedding_layer` layers contain the trained weights. Essentially, they are the latent user and item features learned by `RecommenderNet` and will be used to predict the interaction. As such, while training the neural network to predict rating, the embedding layers are simultaneously trained to extract the embedding user and item features.\n"]},{"cell_type":"markdown","id":"3218e96d-4a16-442e-b15c-df9124117f89","metadata":{},"source":["We can easily get the actual weights using `model.get_layer().get_weights()` methods\n"]},{"cell_type":"code","execution_count":null,"id":"0552f4b0-31d8-4ad1-8498-e875fd3c6006","metadata":{},"outputs":[{"ename":"IndexError","evalue":"list index out of range","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# User features\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m user_latent_features \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muser_embedding_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUser features shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muser_latent_features\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}],"source":["# User features\n","user_latent_features = model.get_layer('user_embedding_layer').get_weights()[0]\n","print(f\"User features shape: {user_latent_features.shape}\")"]},{"cell_type":"code","execution_count":null,"id":"ac63d0b2-3be2-4824-bb1e-7d23ecba558c","metadata":{},"outputs":[],"source":["user_latent_features[0]"]},{"cell_type":"code","execution_count":null,"id":"8271192e-8169-43a1-b2b2-ec212b801a9f","metadata":{},"outputs":[],"source":["item_latent_features = model.get_layer('item_embedding_layer').get_weights()[0]\n","print(f\"Item features shape: {item_latent_features.shape}\")"]},{"cell_type":"code","execution_count":null,"id":"168288c6-bee9-4e1f-a250-af934bc5522c","metadata":{},"outputs":[],"source":["item_latent_features[0]"]},{"cell_type":"markdown","id":"bbe6f129-c84c-4727-89b1-6f9a54da60b5","metadata":{},"source":["Now, each user of the total 33901 users has been transformed into a 16 x 1 latent feature vector and each item of the total 126 has been transformed into a 16 x 1 latent feature vector.\n"]},{"cell_type":"markdown","id":"74baca07-acde-4786-88d6-b65a9415f3ed","metadata":{},"source":["### TASK (Optional): Customize the RecommenderNet to potentially improve the model performance\n"]},{"cell_type":"markdown","id":"6622be2a-5fb7-44b7-9aca-013497180418","metadata":{},"source":["The pre-defined `RecommenderNet()` is a actually very basic neural network, you are encouraged to customize it to see if model prediction performance will be improved. Here are some directions:\n","- Hyperparameter tuning, such as the embedding layer dimensions\n","- Add more hidden layers\n","- Try different activation functions such as `ReLu`\n"]},{"cell_type":"code","execution_count":null,"id":"e20a4b90-090a-465a-b13b-4f2e3c8cbb3a","metadata":{},"outputs":[],"source":["## WRITE YOUR CODE HERE\n","\n","## Update RecommenderNet() class\n","\n","## compile and fit the updated model\n","\n","## evaluate the updated model\n"]},{"cell_type":"markdown","id":"7e440692-1478-4c78-bd9e-a72075d3c48b","metadata":{},"source":["### Summary\n"]},{"cell_type":"markdown","id":"6372a6ab-939e-4021-bde4-f1ff001b8ffa","metadata":{},"source":["In this lab, you have learned and practiced predicting course ratings using neural networks. With a predefined and trained neural network, we can extract or embed users and items into latent feature spaces and further predict the interaction between a user and an item with the latent feature vectors.\n"]},{"cell_type":"markdown","id":"945af885-a6f2-41d5-9202-7d4244ef105e","metadata":{},"source":["## Authors\n"]},{"cell_type":"markdown","id":"72ed86a8-96a1-4ebf-a0da-d6af1741d602","metadata":{},"source":["[Yan Luo](https://www.linkedin.com/in/yan-luo-96288783/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML321ENSkillsNetwork817-2022-01-01)\n"]},{"cell_type":"markdown","id":"45344dbe-00e3-4158-b8a1-14f18834498d","metadata":{},"source":["### Other Contributors\n"]},{"cell_type":"markdown","id":"162ef0e9-adbe-4327-8974-2e05d43ef9d5","metadata":{},"source":["## Change Log\n"]},{"cell_type":"markdown","id":"8928ccd7-368c-4a55-a53d-c3fb3ae6e5b0","metadata":{},"source":["|Date (YYYY-MM-DD)|Version|Changed By|Change Description|\n","|-|-|-|-|\n","|2021-10-25|1.0|Yan|Created the initial version|\n"]},{"cell_type":"markdown","id":"654257e8-413c-41a5-82ab-9c630fb7f3a9","metadata":{},"source":["Copyright © 2021 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":".conda","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":4}
